{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "944122d2",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Bayes’ Theorem offers a method to compute the probability of data belonging to a specific class, given our prior knowledge. The theorem can be expressed as follows:\n",
    "\n",
    "P(class|data) = (P(data|class) * P(class)) / P(data)\n",
    "\n",
    "Here, P(class|data) represents the probability of the class given the provided data.\n",
    "\n",
    "For a comprehensive introduction to Bayes' Theorem, you can refer to the tutorial titled:\n",
    "\n",
    "\"A Gentle Introduction to Bayes' Theorem for Machine Learning.\" https://machinelearningmastery.com/bayes-theorem-for-machine-learning/\n",
    "\n",
    "Naive Bayes is a classification algorithm used for binary (two-class) and multiclass classification tasks. It is known as Naive Bayes or idiot Bayes due to its simplification of probability calculations for each class to ensure tractability.\n",
    "\n",
    "Rather than directly computing the probabilities of each attribute value, Naive Bayes assumes that attributes are conditionally independent, given the class value.\n",
    "\n",
    "This assumption is quite strong and often improbable in real-world data, as it assumes no interaction among attributes. Nevertheless, Naive Bayes performs surprisingly well on data even when this assumption is not met.\n",
    "\n",
    "the goals from this learning is understand:\n",
    "- How to calculate the probabilities required by the Naive Bayes algorithm.\n",
    "- How to implement the Naive Bayes algorithm from scratch.\n",
    "- How to apply Naive Bayes to a real-world predictive modeling problem.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbaee151",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will use iris dataset from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "198b1691-e4c9-4113-9c70-38106ecdf919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80f6314c-3b3f-4592-ba3c-c0c11b39f0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44e4d86-cfe4-4857-bb3f-cb82738d8653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "071bc697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f26c34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c928c3b9-fe8b-415b-a685-c48e2772f47f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data['data']\n",
    "y = data['target']\n",
    "data = np.concatenate((X, y.reshape(-1, 1)), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c8d346d",
   "metadata": {},
   "source": [
    "# Breakdown The Algorithm\n",
    "This Naive Bayes tutorial is organized into 5 distinct parts, each contributing to the implementation of Naive Bayes from scratch and its application in predictive modeling problems:\n",
    "\n",
    "- Step 1: Separate By Class.\n",
    "- Step 2: Summarize Dataset.\n",
    "- Step 3: Summarize Data By Class.\n",
    "- Step 4: Gaussian Probability Density Function.\n",
    "- Step 5: Class Probabilities.\n",
    "\n",
    "By following these steps, you will build a solid foundation to understand and effectively utilize Naive Bayes for your own machine learning tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e63626c",
   "metadata": {},
   "source": [
    "## Step 1: Separate By Class\n",
    "\n",
    "To calculate the probability of data based on their respective class, known as the base rate, we must initially separate our training data by their corresponding classes. This operation is relatively straightforward.\n",
    "\n",
    "We can achieve this by creating a dictionary object where each key represents a class value, and the corresponding value is a list containing all records belonging to that class.\n",
    "\n",
    "Below is a function called separate_by_class() that illustrates this process. It assumes that the last column in each row denotes the class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d77b797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "\tseparated = dict()\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tvector = dataset[i]\n",
    "\t\tclass_value = vector[-1]\n",
    "\t\tif (class_value not in separated):\n",
    "\t\t\tseparated[class_value] = list()\n",
    "\t\tseparated[class_value].append(vector)\n",
    "\treturn separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b60348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "separated = separate_by_class(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bf408f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[5.1 3.5 1.4 0.2 0. ]\n",
      "[4.9 3.  1.4 0.2 0. ]\n",
      "[4.7 3.2 1.3 0.2 0. ]\n",
      "[4.6 3.1 1.5 0.2 0. ]\n",
      "[5.  3.6 1.4 0.2 0. ]\n",
      "[5.4 3.9 1.7 0.4 0. ]\n",
      "[4.6 3.4 1.4 0.3 0. ]\n",
      "[5.  3.4 1.5 0.2 0. ]\n",
      "[4.4 2.9 1.4 0.2 0. ]\n",
      "[4.9 3.1 1.5 0.1 0. ]\n",
      "[5.4 3.7 1.5 0.2 0. ]\n",
      "[4.8 3.4 1.6 0.2 0. ]\n",
      "[4.8 3.  1.4 0.1 0. ]\n",
      "[4.3 3.  1.1 0.1 0. ]\n",
      "[5.8 4.  1.2 0.2 0. ]\n",
      "[5.7 4.4 1.5 0.4 0. ]\n",
      "[5.4 3.9 1.3 0.4 0. ]\n",
      "[5.1 3.5 1.4 0.3 0. ]\n",
      "[5.7 3.8 1.7 0.3 0. ]\n",
      "[5.1 3.8 1.5 0.3 0. ]\n",
      "[5.4 3.4 1.7 0.2 0. ]\n",
      "[5.1 3.7 1.5 0.4 0. ]\n",
      "[4.6 3.6 1.  0.2 0. ]\n",
      "[5.1 3.3 1.7 0.5 0. ]\n",
      "[4.8 3.4 1.9 0.2 0. ]\n",
      "[5.  3.  1.6 0.2 0. ]\n",
      "[5.  3.4 1.6 0.4 0. ]\n",
      "[5.2 3.5 1.5 0.2 0. ]\n",
      "[5.2 3.4 1.4 0.2 0. ]\n",
      "[4.7 3.2 1.6 0.2 0. ]\n",
      "[4.8 3.1 1.6 0.2 0. ]\n",
      "[5.4 3.4 1.5 0.4 0. ]\n",
      "[5.2 4.1 1.5 0.1 0. ]\n",
      "[5.5 4.2 1.4 0.2 0. ]\n",
      "[4.9 3.1 1.5 0.2 0. ]\n",
      "[5.  3.2 1.2 0.2 0. ]\n",
      "[5.5 3.5 1.3 0.2 0. ]\n",
      "[4.9 3.6 1.4 0.1 0. ]\n",
      "[4.4 3.  1.3 0.2 0. ]\n",
      "[5.1 3.4 1.5 0.2 0. ]\n",
      "[5.  3.5 1.3 0.3 0. ]\n",
      "[4.5 2.3 1.3 0.3 0. ]\n",
      "[4.4 3.2 1.3 0.2 0. ]\n",
      "[5.  3.5 1.6 0.6 0. ]\n",
      "[5.1 3.8 1.9 0.4 0. ]\n",
      "[4.8 3.  1.4 0.3 0. ]\n",
      "[5.1 3.8 1.6 0.2 0. ]\n",
      "[4.6 3.2 1.4 0.2 0. ]\n",
      "[5.3 3.7 1.5 0.2 0. ]\n",
      "[5.  3.3 1.4 0.2 0. ]\n",
      "1.0\n",
      "[7.  3.2 4.7 1.4 1. ]\n",
      "[6.4 3.2 4.5 1.5 1. ]\n",
      "[6.9 3.1 4.9 1.5 1. ]\n",
      "[5.5 2.3 4.  1.3 1. ]\n",
      "[6.5 2.8 4.6 1.5 1. ]\n",
      "[5.7 2.8 4.5 1.3 1. ]\n",
      "[6.3 3.3 4.7 1.6 1. ]\n",
      "[4.9 2.4 3.3 1.  1. ]\n",
      "[6.6 2.9 4.6 1.3 1. ]\n",
      "[5.2 2.7 3.9 1.4 1. ]\n",
      "[5.  2.  3.5 1.  1. ]\n",
      "[5.9 3.  4.2 1.5 1. ]\n",
      "[6.  2.2 4.  1.  1. ]\n",
      "[6.1 2.9 4.7 1.4 1. ]\n",
      "[5.6 2.9 3.6 1.3 1. ]\n",
      "[6.7 3.1 4.4 1.4 1. ]\n",
      "[5.6 3.  4.5 1.5 1. ]\n",
      "[5.8 2.7 4.1 1.  1. ]\n",
      "[6.2 2.2 4.5 1.5 1. ]\n",
      "[5.6 2.5 3.9 1.1 1. ]\n",
      "[5.9 3.2 4.8 1.8 1. ]\n",
      "[6.1 2.8 4.  1.3 1. ]\n",
      "[6.3 2.5 4.9 1.5 1. ]\n",
      "[6.1 2.8 4.7 1.2 1. ]\n",
      "[6.4 2.9 4.3 1.3 1. ]\n",
      "[6.6 3.  4.4 1.4 1. ]\n",
      "[6.8 2.8 4.8 1.4 1. ]\n",
      "[6.7 3.  5.  1.7 1. ]\n",
      "[6.  2.9 4.5 1.5 1. ]\n",
      "[5.7 2.6 3.5 1.  1. ]\n",
      "[5.5 2.4 3.8 1.1 1. ]\n",
      "[5.5 2.4 3.7 1.  1. ]\n",
      "[5.8 2.7 3.9 1.2 1. ]\n",
      "[6.  2.7 5.1 1.6 1. ]\n",
      "[5.4 3.  4.5 1.5 1. ]\n",
      "[6.  3.4 4.5 1.6 1. ]\n",
      "[6.7 3.1 4.7 1.5 1. ]\n",
      "[6.3 2.3 4.4 1.3 1. ]\n",
      "[5.6 3.  4.1 1.3 1. ]\n",
      "[5.5 2.5 4.  1.3 1. ]\n",
      "[5.5 2.6 4.4 1.2 1. ]\n",
      "[6.1 3.  4.6 1.4 1. ]\n",
      "[5.8 2.6 4.  1.2 1. ]\n",
      "[5.  2.3 3.3 1.  1. ]\n",
      "[5.6 2.7 4.2 1.3 1. ]\n",
      "[5.7 3.  4.2 1.2 1. ]\n",
      "[5.7 2.9 4.2 1.3 1. ]\n",
      "[6.2 2.9 4.3 1.3 1. ]\n",
      "[5.1 2.5 3.  1.1 1. ]\n",
      "[5.7 2.8 4.1 1.3 1. ]\n",
      "2.0\n",
      "[6.3 3.3 6.  2.5 2. ]\n",
      "[5.8 2.7 5.1 1.9 2. ]\n",
      "[7.1 3.  5.9 2.1 2. ]\n",
      "[6.3 2.9 5.6 1.8 2. ]\n",
      "[6.5 3.  5.8 2.2 2. ]\n",
      "[7.6 3.  6.6 2.1 2. ]\n",
      "[4.9 2.5 4.5 1.7 2. ]\n",
      "[7.3 2.9 6.3 1.8 2. ]\n",
      "[6.7 2.5 5.8 1.8 2. ]\n",
      "[7.2 3.6 6.1 2.5 2. ]\n",
      "[6.5 3.2 5.1 2.  2. ]\n",
      "[6.4 2.7 5.3 1.9 2. ]\n",
      "[6.8 3.  5.5 2.1 2. ]\n",
      "[5.7 2.5 5.  2.  2. ]\n",
      "[5.8 2.8 5.1 2.4 2. ]\n",
      "[6.4 3.2 5.3 2.3 2. ]\n",
      "[6.5 3.  5.5 1.8 2. ]\n",
      "[7.7 3.8 6.7 2.2 2. ]\n",
      "[7.7 2.6 6.9 2.3 2. ]\n",
      "[6.  2.2 5.  1.5 2. ]\n",
      "[6.9 3.2 5.7 2.3 2. ]\n",
      "[5.6 2.8 4.9 2.  2. ]\n",
      "[7.7 2.8 6.7 2.  2. ]\n",
      "[6.3 2.7 4.9 1.8 2. ]\n",
      "[6.7 3.3 5.7 2.1 2. ]\n",
      "[7.2 3.2 6.  1.8 2. ]\n",
      "[6.2 2.8 4.8 1.8 2. ]\n",
      "[6.1 3.  4.9 1.8 2. ]\n",
      "[6.4 2.8 5.6 2.1 2. ]\n",
      "[7.2 3.  5.8 1.6 2. ]\n",
      "[7.4 2.8 6.1 1.9 2. ]\n",
      "[7.9 3.8 6.4 2.  2. ]\n",
      "[6.4 2.8 5.6 2.2 2. ]\n",
      "[6.3 2.8 5.1 1.5 2. ]\n",
      "[6.1 2.6 5.6 1.4 2. ]\n",
      "[7.7 3.  6.1 2.3 2. ]\n",
      "[6.3 3.4 5.6 2.4 2. ]\n",
      "[6.4 3.1 5.5 1.8 2. ]\n",
      "[6.  3.  4.8 1.8 2. ]\n",
      "[6.9 3.1 5.4 2.1 2. ]\n",
      "[6.7 3.1 5.6 2.4 2. ]\n",
      "[6.9 3.1 5.1 2.3 2. ]\n",
      "[5.8 2.7 5.1 1.9 2. ]\n",
      "[6.8 3.2 5.9 2.3 2. ]\n",
      "[6.7 3.3 5.7 2.5 2. ]\n",
      "[6.7 3.  5.2 2.3 2. ]\n",
      "[6.3 2.5 5.  1.9 2. ]\n",
      "[6.5 3.  5.2 2.  2. ]\n",
      "[6.2 3.4 5.4 2.3 2. ]\n",
      "[5.9 3.  5.1 1.8 2. ]\n"
     ]
    }
   ],
   "source": [
    "for label in separated:\n",
    "    print(label)\n",
    "    for row in separated[label]:\n",
    "        print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e269f0a",
   "metadata": {},
   "source": [
    "## Step 2: Summarize Dataset\n",
    "\n",
    "To proceed with calculating probabilities in subsequent steps, we require two essential statistics from the dataset.\n",
    "\n",
    "These statistics are the mean and the standard deviation, which helps us understand the average value and the deviation from that average.\n",
    "\n",
    "The mean, denoted as the average value, can be computed using the formula:\n",
    "\n",
    "mean = sum(x) / n, where x represents the list of values or a specific column we are examining, and n denotes the count of values in that column.\n",
    "\n",
    "Below is a concise function called mean() that calculates the mean of a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1e86201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "\treturn sum(numbers)/float(len(numbers))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47417efe",
   "metadata": {},
   "source": [
    "The sample standard deviation is determined by measuring the mean difference from the mean value. Mathematically, it is computed as:\n",
    "\n",
    "standard deviation = sqrt((sum i to N (x_i – mean(x))^2) / N-1)\n",
    "\n",
    "This formula involves squaring the difference between each data point and the mean, then averaging these squared differences, and finally taking the square root to return the units back to their original scale.\n",
    "\n",
    "Below, you will find a concise function named standard_deviation() that calculates the standard deviation of a list of numbers. Notably, it also computes the mean. For efficiency, you may explore the option of calculating the mean of a list of numbers once and passing it as a parameter to the standard_deviation() function. This optimization can be explored at a later stage if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f9d2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "\treturn sqrt(variance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e24979be",
   "metadata": {},
   "source": [
    "To compute the mean and standard deviation statistics for each input attribute (column) in our dataset, we can follow the following steps:\n",
    "\n",
    "1. Gather all the values of each column into a list.\n",
    "2. Calculate the mean and standard deviation on that list of values.\n",
    "3. Store the statistics for each column in a list or tuple.\n",
    "4. Repeat the above steps for each column in the dataset.\n",
    "5. Return a list of tuples containing the statistics for all columns.\n",
    "\n",
    "Below, you'll find a function called summarize_dataset() that employs Python tricks to reduce the number of lines required for implementation. This function facilitates the calculation of statistics for each column in the dataset and returns a list of tuples containing the mean and standard deviation statistics for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a95097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "\tsummaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "\tdel(summaries[-1])\n",
    "\treturn summaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a956755",
   "metadata": {},
   "source": [
    "The first clever trick involves using the zip() function to aggregate elements from each provided argument. By passing the dataset to the zip() function with the * operator, we can separate the dataset (a list of lists) into separate lists for each row. As a result, the zip() function iterates over each element of each row and returns a column from the dataset as a list of numbers.\n",
    "\n",
    "Subsequently, we calculate the mean, standard deviation, and count of rows for each column. These three numbers are then combined into a tuple, and a list of these tuples is created and stored. To ensure we exclude the statistics for the class variable, we remove the corresponding tuple.\n",
    "\n",
    "Let's put these functions to the test using the contrived dataset mentioned earlier. Below, you'll find the complete example incorporating these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "135035a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5.843333333333335, 0.8280661279778629, 150),\n",
       " (3.057333333333334, 0.435866284936698, 150),\n",
       " (3.7580000000000027, 1.7652982332594667, 150),\n",
       " (1.199333333333334, 0.7622376689603465, 150)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = summarize_dataset(data)\n",
    "summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87c1d70d",
   "metadata": {},
   "source": [
    "Now we are ready to use these functions on each group of rows in our dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68deb84e",
   "metadata": {},
   "source": [
    "## Step 3: Summarize Data By Class\n",
    "\n",
    "In this step, our goal is to obtain relevant statistics from our training dataset, organized by class values.\n",
    "\n",
    "To achieve this, we have already developed two essential functions: \n",
    "\n",
    "1. `separate_by_class()`: This function effectively segregates the dataset into rows based on their respective class labels.\n",
    "\n",
    "2. `summarize_dataset()`: This function calculates summary statistics, such as mean, standard deviation, and count, for each column in the dataset.\n",
    "\n",
    "By combining these two functions, we can conveniently summarize the columns in the dataset, grouped by their class values.\n",
    "\n",
    "For your convenience, the function named `summarize_by_class()` has been created to accomplish this task. It efficiently splits the dataset by class, and then computes statistics for each subset. The resulting statistics are represented as a list of tuples and stored in a dictionary, with each class value serving as the key.\n",
    "\n",
    "In summary, `summarize_by_class()` streamlines the process of organizing the dataset by class values and deriving relevant statistics for each class, which is crucial for various classification tasks, such as building Naive Bayes classifiers or other classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94aa0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset by class then calculate statistics for each row\n",
    "def summarize_by_class(dataset):\n",
    "\tseparated = separate_by_class(dataset)\n",
    "\tsummaries = dict()\n",
    "\tfor class_value, rows in separated.items():\n",
    "\t\tsummaries[class_value] = summarize_dataset(rows)\n",
    "\treturn summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80e744c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "(5.005999999999999, 0.3524896872134512, 50)\n",
      "(3.428000000000001, 0.3790643690962886, 50)\n",
      "(1.4620000000000002, 0.1736639964801841, 50)\n",
      "(0.2459999999999999, 0.10538558938004569, 50)\n",
      "1.0\n",
      "(5.936, 0.5161711470638635, 50)\n",
      "(2.7700000000000005, 0.3137983233784114, 50)\n",
      "(4.26, 0.46991097723995806, 50)\n",
      "(1.3259999999999998, 0.197752680004544, 50)\n",
      "2.0\n",
      "(6.587999999999998, 0.635879593274432, 50)\n",
      "(2.9739999999999998, 0.3224966381726376, 50)\n",
      "(5.552, 0.5518946956639835, 50)\n",
      "(2.026, 0.27465005563666733, 50)\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_by_class(data)\n",
    "for label in summary:\n",
    " print(label)\n",
    " for row in summary[label]:\n",
    "    print(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c08bcaa0",
   "metadata": {},
   "source": [
    "## Step 4: Gaussian Probability Density Function\n",
    "\n",
    "Calculating the probability or likelihood of observing a given real-value, such as X1, can be challenging. However, one approach to estimate this probability is to assume that the X1 values are drawn from a specific distribution, such as a bell curve or Gaussian distribution.\n",
    "\n",
    "A Gaussian distribution can be characterized by just two numbers: the mean and the standard deviation. With these parameters, we can use a mathematical function called the Gaussian Probability Density Function (PDF) to estimate the probability of a given value.\n",
    "\n",
    "The Gaussian PDF can be expressed as follows:\n",
    "\n",
    "f(x) = (1 / sqrt(2 * PI) * sigma) * exp(-((x - mean)^2 / (2 * sigma^2)))\n",
    "\n",
    "Where:\n",
    "- sigma is the standard deviation of the variable x.\n",
    "- mean is the mean value of the variable x.\n",
    "- PI is the mathematical constant representing the value of π (approximately 3.14159).\n",
    "\n",
    "Implementing this function can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d05508ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3989422804014327\n",
      "0.24197072451914337\n",
      "0.24197072451914337\n"
     ]
    }
   ],
   "source": [
    "# Example of Gaussian PDF\n",
    "from math import sqrt\n",
    "from math import pi\n",
    "from math import exp\n",
    "\n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "\texponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "\treturn (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "# Test Gaussian PDF\n",
    "print(calculate_probability(1.0, 1.0, 1.0))\n",
    "print(calculate_probability(2.0, 1.0, 1.0))\n",
    "print(calculate_probability(0.0, 1.0, 1.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f970f0ca",
   "metadata": {},
   "source": [
    "After running the function, it will display the probabilities of certain input values. Notably, when the input value is 1, and both the mean and standard deviation are 1, the input is the most likely, situated at the peak of the bell curve, and has a probability of 0.39.\n",
    "\n",
    "Furthermore, if we maintain the same statistics and modify the input value to be 1 standard deviation away from the mean value, both in the positive and negative directions (i.e., 2 and 0, which are equidistant from the center of the bell curve), the probabilities of these input values will be identical at 0.24."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "242bb2ce",
   "metadata": {},
   "source": [
    "## Step 5: Class Probabilities\n",
    "\n",
    "We will now use the statistics calculated from our training data to estimate probabilities for new data.\n",
    "\n",
    "Probabilities are calculated separately for each class, meaning that we calculate the probability that a new piece of data belongs to the first class, then calculate probabilities for the second class, and so on for all classes.\n",
    "\n",
    "The probability that a piece of data belongs to a class is calculated as follows:\n",
    "\n",
    "P(class|data) = P(X|class) * P(class)\n",
    "\n",
    "You may notice that this is different from the Bayes Theorem described above. The division has been removed to simplify the calculation. Although the result is no longer strictly a probability of the data belonging to a class, we still maximize the value, meaning that the calculation for the class that results in the largest value is taken as the prediction. This is a common implementation simplification, as we are often more interested in the class prediction rather than the probability.\n",
    "\n",
    "The input variables are treated separately, giving the technique its name \"naive\". For the previous example where we had 2 input variables, the calculation of the probability that a row belongs to the first class (class 0) can be calculated as:\n",
    "\n",
    "P(class=0|X1,X2) = P(X1|class=0) * P(X2|class=0) * P(class=0)\n",
    "\n",
    "Now you can see why we need to separate the data by class value. The Gaussian Probability Density function in the previous step is how we calculate the probability of a real value like X1, and the statistics we prepared are used in this calculation.\n",
    "\n",
    "Below is a function named calculate_class_probabilities() that ties all of this together.\n",
    "\n",
    "It takes a set of prepared summaries and a new row as input arguments.\n",
    "\n",
    "First, the total number of training records is calculated from the counts stored in the summary statistics. This is used in the calculation of the probability of a given class (P(class)) as the ratio of rows with a given class to all rows in the training data.\n",
    "\n",
    "Next, probabilities are calculated for each input value in the row using the Gaussian probability density function and the statistics for that column and of that class. Probabilities are multiplied together as they are accumulated.\n",
    "\n",
    "This process is repeated for each class in the dataset.\n",
    "\n",
    "Finally, a dictionary of probabilities is returned with one entry for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bb5b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "\ttotal_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "\tprobabilities = dict()\n",
    "\tfor class_value, class_summaries in summaries.items():\n",
    "\t\tprobabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "\t\tfor i in range(len(class_summaries)):\n",
    "\t\t\tmean, stdev, count = class_summaries[i]\n",
    "\t\t\tprobabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "\treturn probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e2500c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 2.7915339171768885, 1.0: 8.322426199968131e-18, 2.0: 6.008422572010989e-25}\n"
     ]
    }
   ],
   "source": [
    "probabilities = calculate_class_probabilities(summary, data[0])\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "507abcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class for a given row\n",
    "def predict(summaries, row):\n",
    " probabilities = calculate_class_probabilities(summaries, row)\n",
    " best_label, best_prob = None, -1\n",
    " for class_value, probability in probabilities.items():\n",
    "    if best_label is None or probability > best_prob:\n",
    "        best_prob = probability\n",
    "        best_label = class_value\n",
    " return best_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0c212c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = summarize_by_class(data)\n",
    "# define a new record\n",
    "row = [5.0 , 3.5, 1.3, 0.3]\n",
    "# predict the label\n",
    "label = predict(model, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e55a0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data=[5.0, 3.5, 1.3, 0.3], Predicted: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9dab3bd",
   "metadata": {},
   "source": [
    "## Comparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99b5ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data=[5.0, 3.5, 1.3, 0.3], Predicted: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions with Naive Bayes On The Iris Dataset\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "from math import exp\n",
    "from math import pi\n",
    "\n",
    "# Load a CSV file\n",
    "# def load_csv(filename):\n",
    "# \tdataset = list()\n",
    "# \twith open(filename, 'r') as file:\n",
    "# \t\tcsv_reader = reader(file)\n",
    "# \t\tfor row in csv_reader:\n",
    "# \t\t\tif not row:\n",
    "# \t\t\t\tcontinue\n",
    "# \t\t\tdataset.append(row)\n",
    "# \treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "# def str_column_to_float(dataset, column):\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# # Convert string column to integer\n",
    "# def str_column_to_int(dataset, column):\n",
    "# \tclass_values = [row[column] for row in dataset]\n",
    "# \tunique = set(class_values)\n",
    "# \tlookup = dict()\n",
    "# \tfor i, value in enumerate(unique):\n",
    "# \t\tlookup[value] = i\n",
    "# \t\tprint('[%s] => %d' % (value, i))\n",
    "# \tfor row in dataset:\n",
    "# \t\trow[column] = lookup[row[column]]\n",
    "# \treturn lookup\n",
    "\n",
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "\tseparated = dict()\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tvector = dataset[i]\n",
    "\t\tclass_value = vector[-1]\n",
    "\t\tif (class_value not in separated):\n",
    "\t\t\tseparated[class_value] = list()\n",
    "\t\tseparated[class_value].append(vector)\n",
    "\treturn separated\n",
    "\n",
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "\treturn sum(numbers)/float(len(numbers))\n",
    "\n",
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "\treturn sqrt(variance)\n",
    "\n",
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "\tsummaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "\tdel(summaries[-1])\n",
    "\treturn summaries\n",
    "\n",
    "# Split dataset by class then calculate statistics for each row\n",
    "def summarize_by_class(dataset):\n",
    "\tseparated = separate_by_class(dataset)\n",
    "\tsummaries = dict()\n",
    "\tfor class_value, rows in separated.items():\n",
    "\t\tsummaries[class_value] = summarize_dataset(rows)\n",
    "\treturn summaries\n",
    "\n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "\texponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "\treturn (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "\ttotal_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "\tprobabilities = dict()\n",
    "\tfor class_value, class_summaries in summaries.items():\n",
    "\t\tprobabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "\t\tfor i in range(len(class_summaries)):\n",
    "\t\t\tmean, stdev, _ = class_summaries[i]\n",
    "\t\t\tprobabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "\treturn probabilities\n",
    "\n",
    "# Predict the class for a given row\n",
    "def predict(summaries, row):\n",
    "\tprobabilities = calculate_class_probabilities(summaries, row)\n",
    "\tbest_label, best_prob = None, -1\n",
    "\tfor class_value, probability in probabilities.items():\n",
    "\t\tif best_label is None or probability > best_prob:\n",
    "\t\t\tbest_prob = probability\n",
    "\t\t\tbest_label = class_value\n",
    "\treturn best_label\n",
    "\n",
    "# # Make a prediction with Naive Bayes on Iris Dataset\n",
    "# filename = 'iris.csv'\n",
    "# dataset = load_csv(filename)\n",
    "# for i in range(len(dataset[0])-1):\n",
    "# \tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "# str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# fit model\n",
    "model = summarize_by_class(data)\n",
    "# define a new record\n",
    "row = [5. , 3.5, 1.3, 0.3]\n",
    "# predict the label\n",
    "label = predict(model, row)\n",
    "print('Data=%s, Predicted: %s' % (row, label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10d66b6d",
   "metadata": {},
   "source": [
    "# comparation With Scikit Learn Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "70388146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10945d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1873232",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db074d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a5d37aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "77e9e695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Value: 0\n",
      "Predicted Value: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict Output\n",
    "predicted = gnb.predict([X_test[6]])\n",
    "\n",
    "print(\"Actual Value:\", y_test[6])\n",
    "print(\"Predicted Value:\", predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a10d5aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5. , 3.5, 1.3, 0.3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74d34b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9466666666666667\n",
      "F1 Score: 0.9474242424242425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "accuray = accuracy_score(y_pred, y_test)\n",
    "f1 = f1_score(y_pred, y_test, average=\"weighted\")\n",
    "\n",
    "print(\"Accuracy:\", accuray)\n",
    "print(\"F1 Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
